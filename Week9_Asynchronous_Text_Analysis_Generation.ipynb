{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marconoriega0703-sys/MATH_120/blob/main/Week9_Asynchronous_Text_Analysis_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05e899d",
      "metadata": {
        "id": "f05e899d"
      },
      "source": [
        "\n",
        "# Week 9 Asynchronous Assignment — From Data Structures to Text Analysis & Generation\n",
        "\n",
        "**Deliverable:** Submit this completed notebook on GitHub (`assignments/week9/`).\n",
        "\n",
        "---\n",
        "\n",
        "## 🧭 Before You Begin: Background Resources (15–25 min total)\n",
        "\n",
        "### 📘 Readings\n",
        "\n",
        "* **“N-grams and Markov Chains” (decontextualize)** — https://www.decontextualize.com/teaching/rwet/n-grams-and-markov-chains/  \n",
        "\n",
        "### 🎥 Short videos\n",
        "1. Introducing Markov Chains - https://youtu.be/JHwyHIz6a8A?si=mlbkGrhewP2eJJoT\n",
        "\n",
        "2. Introduction to N-Grams - https://www.youtube.com/watch?v=hM49MPmakNI\n",
        "\n",
        "\n",
        "### 🎧 Optional podcast context\n",
        "- Radiolab - [Talking to Machines](https://open.spotify.com/episode/2XCSL3ntSjclhBQPAFg6oS?si=2c8bdd2f07924eaa)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35feaf62",
      "metadata": {
        "id": "35feaf62"
      },
      "source": [
        "\n",
        "## Learning objectives\n",
        "- Use **lists**, **tuples**, and **dictionaries** to organize and analyze text.\n",
        "- Build a **token frequency table** and **bigram / n-gram** index.\n",
        "- Implement a simple **Markov (n-gram) text generator** using tuple contexts.\n",
        "- Reflect on **limitations and ethics** of text generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e64be201",
      "metadata": {
        "id": "e64be201"
      },
      "source": [
        "\n",
        "## Setup\n",
        "\n",
        "Run the cell below to set up helper functions and a small sample corpus.  \n",
        "You may replace `CORPUS_TEXT` with your own **original writing** (short paragraph).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "25261f58",
      "metadata": {
        "id": "25261f58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86059431-8739-4bbe-f568-b42d65e4b5f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preview tokens: ['all', 'children', 'grow', 'up', ',', 'except', 'one', '.', 'they', 'soon', 'know', 'that', 'they', 'will', 'grow'] ...\n"
          ]
        }
      ],
      "source": [
        "# --- SETUP ---\n",
        "import re, random\n",
        "from collections import Counter\n",
        "\n",
        "# A small sample corpus; replace with your own short text if you prefer.\n",
        "CORPUS_TEXT = (\n",
        "    \"all children grow up, except one. they soon know that they will grow up, \"\n",
        "    \"and the way they know it is that they begin to be like other people. \"\n",
        "    \"it is then that, if you are careful, you can see it happening.\"\n",
        ")\n",
        "\n",
        "def normalize(text):\n",
        "    # Lowercase, separate punctuation, collapse whitespace, split on spaces\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'([.,;:!?()])', r' \\1 ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text.split(' ')\n",
        "\n",
        "def make_unigram_counts(tokens):\n",
        "    return Counter(tokens)\n",
        "\n",
        "def make_next_tokens(tokens):\n",
        "    d = {}\n",
        "    for i in range(len(tokens)-1):\n",
        "        cur, nxt = tokens[i], tokens[i+1]\n",
        "        d.setdefault(cur, []).append(nxt)\n",
        "    return d\n",
        "\n",
        "def make_ngram_index(tokens, n=2):\n",
        "    # Map tuple context of length n -> list of next tokens\n",
        "    index = {}\n",
        "    for i in range(len(tokens)-n):\n",
        "        ctx = tuple(tokens[i:i+n])\n",
        "        nxt = tokens[i+n]\n",
        "        index.setdefault(ctx, []).append(nxt)\n",
        "    return index\n",
        "\n",
        "def choose_weighted(counter_like):\n",
        "    # counter_like can be an iterable of tokens or a dict mapping token->count\n",
        "    c = Counter(counter_like)\n",
        "    items, weights = zip(*c.items())\n",
        "    return random.choices(items, weights=weights, k=1)[0]\n",
        "\n",
        "def generate_markov(index, seed, length=40):\n",
        "    n = len(seed)\n",
        "    out = list(seed)\n",
        "    while len(out) < length:\n",
        "        ctx = tuple(out[-n:])\n",
        "        options = index.get(ctx)\n",
        "        if not options:\n",
        "            break\n",
        "        out.append(choose_weighted(options))\n",
        "    return \" \".join(out)\n",
        "\n",
        "tokens = normalize(CORPUS_TEXT)\n",
        "print(\"Preview tokens:\", tokens[:15], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d50d8b7a",
      "metadata": {
        "id": "d50d8b7a"
      },
      "source": [
        "\n",
        "## Part 1 — Quick review (tuples, lists, dictionaries)  \n",
        "\n",
        "**Your turn 1.1**  \n",
        "- Create `seed_bigram`: a **tuple** with the first 2 tokens.  \n",
        "- Create `rest`: a **list** of the remaining tokens.  \n",
        "- Create `pos_map`: a **dict** mapping each unique token → its **first** index in `tokens`.  \n",
        "- Briefly explain (in a comment) why a **tuple** (immutable) is a good choice for n-gram contexts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "db21b6fc",
      "metadata": {
        "id": "db21b6fc"
      },
      "outputs": [],
      "source": [
        "# TODO: Your turn 1.1\n",
        "seed_bigram = tuple(tokens[:2])\n",
        "rest = list[tokens[2:]]\n",
        "pos_map = {token: tokens.index(token) for token in set(tokens)}\n",
        "# Tuples are preferred over lists and dicts to represent n-grams as tuples can be used as a dictionary key.\n",
        "# A dictionary key must be immutable and tuples are immutable, while lists and dicts are not."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuples are preferred over lists and dicts to represent n-grams as tuples can be used as a dictionary key. A dictionary key must be immutable and tuples are immutable, while lists and dicts are not."
      ],
      "metadata": {
        "id": "PrxolBjq11sK"
      },
      "id": "PrxolBjq11sK"
    },
    {
      "cell_type": "markdown",
      "id": "81607830",
      "metadata": {
        "id": "81607830"
      },
      "source": [
        "\n",
        "## Part 2 — Frequency analysis\n",
        "\n",
        "**Your turn 2.1**  \n",
        "- Build `freq` = dict/Counter of **token → count**.  \n",
        "- Show the **top 10** tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "105e2f97",
      "metadata": {
        "id": "105e2f97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daabc0ce-626e-48e5-c14e-e8e060eb9c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ",: 4\n",
            "they: 4\n",
            ".: 3\n",
            "that: 3\n",
            "it: 3\n",
            "grow: 2\n",
            "up: 2\n",
            "know: 2\n",
            "is: 2\n",
            "you: 2\n"
          ]
        }
      ],
      "source": [
        "# TODO: Your turn 2.1\n",
        "freq = Counter(tokens) # freq counts the number of times a token appears.\n",
        "for token, count in freq.most_common(10): # most_common(n) returns the n most common tokens in the corpus.\n",
        "    print(f\"{token}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "905ba2c1",
      "metadata": {
        "id": "905ba2c1"
      },
      "source": [
        "\n",
        "## Part 3 — Bigram next-token index\n",
        "**Your turn 3.1**  \n",
        "- Build `next_tokens` mapping token → list of tokens that **follow** it somewhere.  \n",
        "- Print the first 5 entries in the form `token -> [sample next tokens]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "2245ee0b",
      "metadata": {
        "id": "2245ee0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62926a9b-bdfe-4c1f-8dad-14872e0984ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all -> ['children']\n",
            "children -> ['grow']\n",
            "grow -> ['up', 'up']\n",
            "up -> [',', ',']\n",
            ", -> ['except', 'and', 'if', 'you']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Your turn 3.1\n",
        "next_tokens = make_next_tokens(tokens)\n",
        "for token, next_token in list(next_tokens.items())[:5]: # The entry in [] is the next token whenever the word appears in the corpus.\n",
        "    print(f\"{token} -> {next_token}\") # This is why grow has two entries of \"up\" in [] as grow appears twice in the text with \"up\" being the next token both times."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "039b3513",
      "metadata": {
        "id": "039b3513"
      },
      "source": [
        "\n",
        "## Part 4 — General n-grams with tuple keys\n",
        "\n",
        "**Your turn 4.1**  \n",
        "- Use `make_ngram_index(tokens, n=2)` and `n=3`.  \n",
        "- Inspect a few keys and values; explain how tuples serve as fixed-length contexts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "fbbe3f1e",
      "metadata": {
        "id": "fbbe3f1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0495e425-80e4-420a-cfa0-13a4614d79c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('all', 'children') -> ['grow']\n",
            "('children', 'grow') -> ['up']\n",
            "('grow', 'up') -> [',', ',']\n",
            "('up', ',') -> ['except', 'and']\n",
            "(',', 'except') -> ['one']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Your turn 4.1\n",
        "ngram_index = make_ngram_index(tokens, n=2)\n",
        "for key, value in tuple(ngram_index.items())[:5]: # The entry in [] is the next token that is after two specific tokens.\n",
        "    print(f\"{key} -> {value}\") # Ex: \",\" appears after \"up\" twice with \"except\" being the next token in the first instance and \"and\" being the next token in the second."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_index = make_ngram_index(tokens, n=3)\n",
        "for key, value in tuple(ngram_index.items())[:5]:\n",
        "    print(f\"{key} -> {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLdw0pEz6Jsj",
        "outputId": "0957e177-20b9-450d-87b2-0175561329d4"
      },
      "id": "hLdw0pEz6Jsj",
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('all', 'children', 'grow') -> ['up']\n",
            "('children', 'grow', 'up') -> [',']\n",
            "('grow', 'up', ',') -> ['except', 'and']\n",
            "('up', ',', 'except') -> ['one']\n",
            "(',', 'except', 'one') -> ['.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuples serve as fixed-length contexts as they are immutable. Because of this, the length of the key will always be the value you assign to n."
      ],
      "metadata": {
        "id": "eOUpWCVn6beF"
      },
      "id": "eOUpWCVn6beF"
    },
    {
      "cell_type": "markdown",
      "id": "45d329d4",
      "metadata": {
        "id": "45d329d4"
      },
      "source": [
        "\n",
        "## Part 5 — Tiny Markov text generator\n",
        "\n",
        "**Your turn 5.1**  \n",
        "- Use the **bigram** model first (`n=2`).  \n",
        "- Pick a `seed` (tuple of length 2) from your tokens.  \n",
        "- Generate 30–50 tokens.  \n",
        "- Try again with `n=3` and compare.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "a14807b0",
      "metadata": {
        "id": "a14807b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7027049c-d245-48d7-8ba4-a1bbd7d3f9c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all children grow up , and the way they know it is then that , if you are careful , you can see it happening .\n"
          ]
        }
      ],
      "source": [
        "# TODO: Your turn 5.1\n",
        "bigram_index = make_ngram_index(tokens, n=2)\n",
        "seed = tuple(tokens[:2])\n",
        "print(generate_markov(bigram_index, seed, length=30))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_index = make_ngram_index(tokens, n=2)\n",
        "seed = tuple(tokens[:2])\n",
        "print(generate_markov(bigram_index, seed, length=40))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swm1KJFS-5a7",
        "outputId": "c4488ecc-4a89-492a-fb38-3a8b9819b547"
      },
      "id": "swm1KJFS-5a7",
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all children grow up , except one . they soon know that they will grow up , and the way they know it is that they will grow up , and the way they know it is that they will\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_index = make_ngram_index(tokens, n=3)\n",
        "seed = tuple(tokens[:3])\n",
        "print(generate_markov(bigram_index, seed, length=30))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU6jKelN-lQx",
        "outputId": "bdca37ee-7a50-4295-d694-dfcee2621f0a"
      },
      "id": "AU6jKelN-lQx",
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all children grow up , except one . they soon know that they will grow up , and the way they know it is that they begin to be like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_index = make_ngram_index(tokens, n=3)\n",
        "seed = tuple(tokens[:3])\n",
        "print(generate_markov(bigram_index, seed, length=40))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va9sJy4j-9lY",
        "outputId": "3c92ce92-cd47-44f4-93ab-7e471eb233b8"
      },
      "id": "Va9sJy4j-9lY",
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all children grow up , and the way they know it is that they begin to be like other people . it is then that , if you are careful , you can see it happening .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Markov Model with the order of 3 has more comprehensible text than the model that has an order of 2 as the model with the higher order considers more tokens meaning that it's more likely to idenify patterns of the text."
      ],
      "metadata": {
        "id": "ZU89ClpmBysT"
      },
      "id": "ZU89ClpmBysT"
    },
    {
      "cell_type": "markdown",
      "id": "3fac3405",
      "metadata": {
        "id": "3fac3405"
      },
      "source": [
        "\n",
        "## Reflection\n",
        "\n",
        "- One **advantage** and one **limitation** of an n-gram Markov model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "543d1d64",
      "metadata": {
        "id": "543d1d64"
      },
      "outputs": [],
      "source": [
        "reflection = \"\"\"\n",
        "An advantage of a Markov model is that the model will always return an output as the model will move to the next iteration if current iteration can't find the n-gram.\n",
        "A limitation of a Markov model is that the model is not reliable for generating longer text as it's predictions are based on small sequences of tokens, not the entire context of said text.\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}